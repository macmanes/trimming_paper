\documentclass[10.5pt]{article}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, citecolor=black}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{setspace}
\begin{document}
\begin{flushleft}
{\large
\textbf{On the optimal trimming of high-throughput mRNA sequence data}
}

\vspace{8mm}

\noindent
Matthew D MacManes$^{1,2}$$^\ast$\\
\vspace{4mm}


\bf{1} \textnormal{\em{University of California, Berkeley. Berkeley, CA 94720}}  \\
\bf{2} \textnormal{\em{California Institute of Quantitative Biology}} \\


\vspace{4mm}
 
\bf{$\ast$} \textnormal{Corresponding author: \href{mailto:macmanes@gmail.com}{macmanes@gmail.com}, Twitter: \href{https://twitter.com/PeroMHC}{$@$PeroMHC}}
\end{flushleft}

\vspace{8mm}


\begin{abstract}
The widespread and rapid adoption of high-throughput sequencing technologies has changed the face of modern studies of evolutionary genetics. Indeed, newer sequencing technologies, like Illumina sequencing, have afforded researchers the opportunity to gain a deep understanding of genome level processes that underlie evolutionary change. In particular, researchers interested in functional biology and adaptation have used these technologies to sequence mRNA transcriptomes of specific tissues, which in turn are often compared to other tissues, or other individuals with different phenotypes.  While these techniques are extremely powerful, careful attention to data quality is required. In particular, because high-throughput sequencing is more error-prone than traditional Sanger sequencing, quality trimming of sequence reads should be an important step in all data processing pipelines.  While several software packages for quality trimming exist, no general guidelines for the specifics of trimming have been developed. Here, using both simulated and empirically derived sequence data, as well as several of the available read-trimmers, I provide general recommendations regarding the optimal strength of trimming, specifically in mRNA-Seq studies.  Although very aggressive quality trimming is common, this study suggests that a more gentle trimming, specifically of those nucleotides whose \textsc{Phred} score \textless 5, is superior.     
\end{abstract}

\doublespacing
\linenumbers
\begin{flushleft}
\vspace{12mm}

\section*{Introduction}
The popularity of genome--enabled biology has increased dramatically, particularly for researchers studying non-model organisms, over the last few years.  For many, the primary goal of these works is to better understand the genomic underpinnings of adaptive \citep{Linnen:2013ir,Narum:2013kc} or functional \citep{MunozMerida:2013dw,Hsu:2012dg} traits. While extremely promising, the study of functional genomics in non-model organisms typically requires the generation of a reference transcriptome to which comparisons are made.  Although compared to genome assembly \citep{Bradnam:2013uu,Earl:2011gt}. transcriptome assembly is less challenging, significant computational hurdles still exist. Amongst the most difficult of challenges involves the reconstruction of isoforms \citep{Pyrkosz:2013tm} and simultaneous assembly of transcripts where read coverage (=expression) varies by orders of magnitude.  \\

\noindent
These processes are further complicated by the error-prone nature of high-throughput sequencing reads.  With regards to Illumina sequencing, error is distributed non-randomly over the length of the read, with the rate of error increasing from 5' to 3' end \citep{Liu:2012iv}. These errors are overwhelmingly substitution errors \citep{Yang:2013ck}, with the global error rate being between 1\% and 3\%. Although \textit{de Bruijn} graph assemblers do a remarkable job in distinguishing error from correct sequence, sequence error does results in assembly error.  While this type of error is problematic for all studies, it may be particularly troublesome for SNP-based population genetic studies. In addition to the biological concerns, sequencing read error may results in problems of a more technical importance. Because most transcriptome assemblers use a \textit{de Bruijn} graph representation of sequence connectedness, sequencing error can dramatically increase the size and complexity of the graph, and thus increase both RAM requirements and runtime.  \\

\noindent
In addition to sequence error correction, which has been shown to improved accuracy of the \textit{de novo} assembly \cite{MacManes:2013tva}, low quality (=high probability of error) nucleotides are commonly removed from the sequencing reads prior to assembly, using one of several available tools (\textsc{Trimmomatic} \citep{Lohse:2012fg}, \textsc{Fastx Toolkit} (\url{http://hannonlab.cshl.edu/fastx_toolkit/index.html}), \textsc{biopieces} (\url{http://www.biopieces.org/}), \textsc{SolexaQA} \citep{Cox:2010ch}). These tools typically use a sliding window approach,  discarding nucleotides falling below a given (user selected) average quality threshold.  The trimmed dataset that remains will undoubtedly contain error, though the absolute number will surely be decreased. \\

\noindent
Although the process of nucleotide quality trimming is commonplace in HTS analysis pipelines, it's optimal implementation has not been well defined. Though the rigor with which trimming is performed may be guided by the design of the experiment, a deeper understanding of the effects of trimming is desirable. As transcriptome-based studies of functional genomics continue to become more popular, understanding how quality trimming of mRNA-seq reads used in these types of experiments is urgently needed. Researchers currently working in these field appear to favor aggressive trimming (e.g. \citep{Riesgo:2011do,Looso:2013eu}), but this may not be optimal. Indeed, one can easily image aggressive trimming resulting in the removal of a large amout of high quality data (Even nucleotides removed with the commonly used  \textsc{Phred}=20 threshold are accurate 99\% of the time), just as lackadaisical trimming (or no trimming) may result in nucleotide errors being incorporated into the assembled transcriptome.  \\

\noindent
Here, I attempt to provide recommendations regarding the efficient trimming of high-throughput sequence reads, specifically or mRNASeq reads from the Illumina platform. To do this, I used both simulated reads from the \textit{Mus musculus} transcriptome, as well as an empirically derived dataset.  These datasets were trimmed at various levels of stringency, assembled, then analyzed for assembly error. These results aim to guide researchers through this critical aspect of the analysis of high-throughput sequence data. While the results of this paper may not be applicable to all studies, that so many researchers are interested in the genomics of adaptation and phenotypic diversity suggests its widespread utility.  

\section*{Materials and Methods}
\noindent
Because I was interested in understanding the effects of sequence read quality trimming on the assembly of vertebrate transcriptome assembly, we elected to simulate thirty million 100nt paired-end Illumina reads with the program \textsc{Flux Simulator} \citep{Griebel:2012ti}.  This dataset is fully described in a previous publication \citep{MacManes:2013tva}. In addition to this simulated dataset, an empirically derived dataset consisting of 30 million 76nt paired-end mRNA Illumina reads, which is a subset of the well-characterized 50M read set continued within the \textsc{Trinity} software package. Quality metrics for the raw and quality trimmed reads were generated using the program \textsc{SolexaQA} \citep{Cox:2010ch}, and visualized using R \citep{RALanguageandEn:wf}. \\


\noindent
Both simulated and empirical datasets were trimmed at varying quality thresholds using the software packages \textsc{Trimmomatic} \citep{Lohse:2012fg}. Specifically, sequences were trimmed at both 5' and 3' ends using \textsc{Phred} $\leq$ 0, $\leq$ 1, $\leq$ 5, $\leq$ 10, and $\leq$ 20. Though adapter trimming is included in the functionality the \textsc{Trimmomatic} packages, because the simulated dataset was produced sans adapter contamination-- for the purposes of comparison, the empirical dataset was cleansed of adapter before quality trimming. \\

\noindent
Transcriptome assemblies were generated using the default settings of the program \textsc{Trinity} \citep{Haas:jq,Grabherr:2011jb}.  Code for running \textsc{Trinity} is available at \url{https://gist.github.com/macmanes/5859956}. Assemblies were evaluated using a variety of different metrics. First, \textsc{Blast+} \citep{Camacho:2009fc} was used to match assembled transcripts to their reference.  \textsc{TransDecoder} (\url{http://transdecoder.sourceforge.net/}) was used to identify full-length transcripts. The program \textsc{Blat} \citep{Kent:2002jd} was used to identify and count nucleotide mismatches between reconstructed transcripts and their corresponding reference.  Differences were visualized using the program R. \\

\section*{Results}

\section*{Discussion}

\section*{Acknowledgments}


\singlespacing
\bibliographystyle{model2-names.bst}
\bibliography{formatted.bib}

\section*{Figures \& Tables}

\textbf{\hypertarget{Figure 1}{Figure 1}} \\
\centerline{\includegraphics[width=40.0\baselineskip]{Figure1a.eps}}


\noindent
Fig. 1

\textbf{\hypertarget{Figure 2}{Figure 2}} \\
\centerline{\includegraphics[width=40.0\baselineskip]{Figure2.eps}}

\noindent
Fig. 2 
\end{flushleft}
\end{document}


